{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNslsR5vjIppXv/tw1THKQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anschelalmog/SDRML_project/blob/main/project_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: center\">\n",
        "    <img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/logo.png?raw=true' width=800/>  \n",
        "</div>"
      ],
      "metadata": {
        "id": "bjXz6R6zqwfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sequential Decision Making and Reinforcement Learning (SDRML) Final Project**"
      ],
      "metadata": {
        "id": "_l3Dy-pLixQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ‘¥ **Team Members**\n",
        "| **Name** | **ID** |\n",
        "|:---------------:|:----------:|\n",
        "| [Almog Anschel](mailto:anschelalmog@campus.technion.ac.il) | 316353531 |\n",
        "| [Eden](mailto:eden@campus.technion.ac.il) | 123456778 |"
      ],
      "metadata": {
        "id": "TiehJSiMi-6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ðŸ“Œ [GitHub Repo & Paper](https://github.com/anschelalmog/SDRML_project.git)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GG_TtmAQrp8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=ZGqV6cHUtDmj&format=png&color=000000\" style=\"height:50px;display:inline\"> Imports and Utillity Functions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_PO39rgBkDuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "rgA8rjoVl9FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install torch gymnasium tensorboardX matplotlib pyyaml\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "##################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "##################\n",
        "import gymnasium as gym\n",
        "from gymnasium.envs.registration import register\n",
        "from gymnasium import spaces, utils\n",
        "\n"
      ],
      "metadata": {
        "id": "bsQZwe9uyfKe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "asQ5UIbnlcwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "print(\"Random seed set.\")\n",
        "\n",
        "\n",
        "register(id='ElectricityMarket-v0',\n",
        "         entry_point='__main__:ElectricityMarketEnv',\n",
        "         kwargs={'args': None},\n",
        "         max_episode_steps=200)\n",
        "\n",
        "# Setup TensorBoard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Function to launch TensorBoard\n",
        "log_dir = \"logs/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSlXpih8lfp-",
        "outputId": "21868b92-11fd-464f-a95d-5f97a1fc0180"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Random seed set.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment ElectricityMarket-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=ZKacxH3j7_2b&format=png&color=000000\" style=\"height:50px;display:inline\"> Environment"
      ],
      "metadata": {
        "id": "Kh8TPyQDkHwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **ElectricityMarketEnv** is an environment for simulating an electricity market with a battery storage system.\n",
        "\n",
        "The agent interacts with the environment by taking continuous actions, charging (positive) or discharging (negative) a battery.\n",
        "\n",
        "The environment models the dynamics of household electricity demand and market price, both following periodic functions with noise.\n",
        "\n",
        "---\n",
        "\n",
        "## **States**\n",
        "- **$SoC$** : State of Charge - current energy level in the battery.\n",
        "- **$ D_t$**: Household electricity demand at the current timestep.\n",
        "- **$ P_t$**: Market price of electricity at the current timestep.\n",
        "\n",
        "\n",
        "## **Action**\n",
        "- A continuous value in **$[- \\text{battery_cap}, \\text{battery_cap}]$**.\n",
        "\n",
        "\n",
        "## **Reward (Options):**\n",
        "\n",
        "1.    **profit:** (default) $R = P_t \\times \\text{sold_energy}$\n",
        "- Rewards the agent solely based on the revenue generated from selling surplus energy after meeting demand.\n",
        "2. **penalty unmet:** $R = P_t \\times \\text{discharge_amount} - \\lambda \\times \\max(0, D_t - \\text{discharge_amount})$\n",
        "- Rewards all discharged energy but applies a **penalty** for any portion of household demand that is not met.\n",
        "- This encourages the agent to **prioritize internal demand** before selling energy.\n",
        "3. **degradation:**\n",
        "$ R = P_t \\times \\text{sold_energy} - c \\times \\text{discharge_amount} $\n",
        "- Rewards surplus energy sold while **subtracting a cost** proportional to the total discharged energy to account for **battery degradation**.\n"
      ],
      "metadata": {
        "id": "DL0Vtb27DWPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ElectricityMarketEnv(gym.Env):\n",
        "    def __init__(self, args=None):\n",
        "        super(ElectricityMarketEnv, self).__init__()\n",
        "\n",
        "        # --------------------------\n",
        "        # Battery parameters\n",
        "        # --------------------------\n",
        "        self.battery_capacity = 100.0  # Maximum energy that can be stored\n",
        "        self.initial_soc = 50.0  # Initial State of Charge\n",
        "        self.soc = self.initial_soc  # Current SoC\n",
        "\n",
        "        # --------------------------\n",
        "        # Environment dynamics parameters\n",
        "        # --------------------------\n",
        "        self.max_steps = 200  # Total timesteps in one episode\n",
        "        self.current_step = 0  # Timestep counter\n",
        "\n",
        "        # --------------------------\n",
        "        # Define the action space.\n",
        "        # The agent's action is a continuous value in [-battery_capacity, battery_capacity].\n",
        "        # A positive value represents charging and a negative value represents discharging.\n",
        "        # --------------------------\n",
        "        self.action_space = spaces.Box(low=-self.battery_capacity,\n",
        "                                       high=self.battery_capacity,\n",
        "                                       shape=(1,),\n",
        "                                       dtype=np.float32)\n",
        "\n",
        "        # --------------------------\n",
        "        # Define the observation space.\n",
        "        # The state consists of [SoC, Demand, Price].\n",
        "        # SoC is bounded between 0 and battery_capacity.\n",
        "        # Demand and Price are non-negative; we use a very high upper bound.\n",
        "        # --------------------------\n",
        "        obs_low = np.array([0.0, 0.0, 0.0], dtype=np.float32)\n",
        "        obs_high = np.array([self.battery_capacity, np.finfo(np.float32).max, np.finfo(np.float32).max],\n",
        "                            dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
        "\n",
        "        # --------------------------\n",
        "        # Reward function selection.\n",
        "        # Default is \"profit\", but you can choose \"penalty_unmet\" or \"degradation\" via the args parameter.\n",
        "        # --------------------------\n",
        "        self.reward_type = \"profit\"\n",
        "        if args is not None and hasattr(args, 'reward_type'):\n",
        "            self.reward_type = args.reward_type\n",
        "            #logger.info(\n",
        "             #   f\"ElectricityMarketEnv initialized with battery_capacity={self.battery_capacity}, initial_soc={self.initial_soc}, max_steps={self.max_steps}, reward_type={self.reward_type}\")\n",
        "\n",
        "        # --------------------------\n",
        "        # Set the random seed for reproducibility.\n",
        "        # --------------------------\n",
        "        self.seed()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        \"\"\"\n",
        "        Set the seed for random number generation.\n",
        "        \"\"\"\n",
        "        self.np_random, seed = utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, seed=42, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state at the start of an episode.\n",
        "        Returns a tuple (observation, info) as required by Gymnasium.\n",
        "        \"\"\"\n",
        "        self.soc = self.initial_soc\n",
        "        self.current_step = 0\n",
        "        # logger.info(f\"Environment reset: initial SoC={self.soc}\")\n",
        "        obs = self._get_obs()\n",
        "        return obs, {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"\n",
        "        Construct the current observation.\n",
        "        The observation includes the current SoC, the demand, and the price.\n",
        "        Demand and price are functions of normalized time.\n",
        "        \"\"\"\n",
        "        t_norm = self.current_step / self.max_steps  # Normalized time [0, 1]\n",
        "        demand = self._demand_function(t_norm)\n",
        "        price = self._price_function(t_norm)\n",
        "        obs = np.array([self.soc, demand, price], dtype=np.float32)\n",
        "        # logger.debug(f\"Observation: SoC={self.soc}, Demand={demand:.2f}, Price={price:.2f}\")\n",
        "        return obs\n",
        "\n",
        "    def _demand_function(self, t):\n",
        "        \"\"\"\n",
        "        Compute the household electricity demand at time t.\n",
        "\n",
        "        The demand function is modeled as a combination of two Gaussian functions:\n",
        "\n",
        "        \\[\n",
        "        f_D(t) = 100 \\cdot \\exp\\left(-\\frac{(t-0.4)^2}{2 \\cdot (0.05)^2}\\right)\n",
        "                + 120 \\cdot \\exp\\left(-\\frac{(t-0.7)^2}{2 \\cdot (0.1)^2}\\right)\n",
        "        \\]\n",
        "\n",
        "        A small Gaussian noise (mean 0, std 5.0) is added to simulate randomness.\n",
        "        \"\"\"\n",
        "        base_demand = (100.0 * np.exp(-((t - 0.4) ** 2) / (2 * (0.05 ** 2))) +\n",
        "                       120.0 * np.exp(-((t - 0.7) ** 2) / (2 * (0.1 ** 2))))\n",
        "        noise = self.np_random.normal(0, 5.0)  # Noise term\n",
        "        demand = max(base_demand + noise, 0.0)  # Ensure demand is non-negative\n",
        "        # logger.debug(f\"Computed demand={demand:.2f} at t={t:.2f}\")\n",
        "\n",
        "        return demand\n",
        "\n",
        "    def _price_function(self, t):\n",
        "        \"\"\"\n",
        "        Compute the market price of electricity at time t.\n",
        "\n",
        "        The price function is modeled as a combination of two Gaussian functions:\n",
        "\n",
        "        \\[\n",
        "        f_P(t) = 50 \\cdot \\exp\\left(-\\frac{(t-0.3)^2}{2 \\cdot (0.07)^2}\\right)\n",
        "                + 80 \\cdot \\exp\\left(-\\frac{(t-0.8)^2}{2 \\cdot (0.08)^2}\\right)\n",
        "        \\]\n",
        "\n",
        "        A small Gaussian noise (mean 0, std 2.0) is added to simulate randomness.\n",
        "        \"\"\"\n",
        "        base_price = (50.0 * np.exp(-((t - 0.3) ** 2) / (2 * (0.07 ** 2))) +\n",
        "                      80.0 * np.exp(-((t - 0.8) ** 2) / (2 * (0.08 ** 2))))\n",
        "        noise = self.np_random.normal(0, 2.0)  # Noise term\n",
        "        price = max(base_price + noise, 0.0)  # Ensure price is non-negative\n",
        "        # logger.debug(f\"Computed price={price:.2f} at t={t:.2f}\")\n",
        "        return price\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one timestep in the environment.\n",
        "\n",
        "        Parameters:\n",
        "            action (array): A 1D numpy array with one element representing the\n",
        "                            amount of energy to charge (positive) or discharge (negative).\n",
        "\n",
        "        Returns:\n",
        "            obs (array): Next observation ([SoC, demand, price]).\n",
        "            reward (float): Reward earned this timestep.\n",
        "            done (bool): Whether the episode has ended.\n",
        "            info (dict): Additional info (empty in this implementation).\n",
        "        \"\"\"\n",
        "        # Clip the action within allowed bounds.\n",
        "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
        "        action_value = action[0]  # Extract scalar from array\n",
        "        info = {}\n",
        "\n",
        "        # Compute current demand and price based on normalized time.\n",
        "        t_norm = self.current_step / self.max_steps\n",
        "        demand = self._demand_function(t_norm)\n",
        "        price = self._price_function(t_norm)\n",
        "\n",
        "        if action_value >= 0:\n",
        "            # --------------------------\n",
        "            # Charging the battery.\n",
        "            # The battery cannot be charged beyond its capacity.\n",
        "            # --------------------------\n",
        "            charge_amount = min(action_value, self.battery_capacity - self.soc)\n",
        "            self.soc += charge_amount\n",
        "            reward = 0.0  # No immediate reward for charging\n",
        "            # logger.info(\n",
        "                #f\"Step {self.current_step}: Charging battery by {charge_amount:.2f} kWh, New SoC={self.soc:.2f}\")\n",
        "\n",
        "        else:\n",
        "            # --------------------------\n",
        "            # Discharging the battery.\n",
        "            # First, limit the discharge to the available energy in the battery.\n",
        "            # --------------------------\n",
        "            discharge_requested = -action_value  # Convert to a positive value\n",
        "            discharge_possible = min(discharge_requested, self.soc)\n",
        "            self.soc -= discharge_possible\n",
        "\n",
        "            # --------------------------\n",
        "            # Use discharged energy to meet the household demand.\n",
        "            # Any energy beyond meeting the demand is sold to the grid.\n",
        "            # --------------------------\n",
        "            if discharge_possible >= demand:\n",
        "                sold_energy = discharge_possible - demand\n",
        "            else:\n",
        "                sold_energy = 0.0  # Not enough to cover demand, so nothing is sold\n",
        "\n",
        "            # Compute reward based on the chosen reward function.\n",
        "            reward = self._compute_reward(demand, sold_energy, price, discharge_possible)\n",
        "            # logger.info(\n",
        "               # f\"Step {self.current_step}: Discharged {discharge_possible:.2f} kWh, Demand={demand:.2f}, Sold Energy={sold_energy:.2f}, Reward={reward:.2f}\")\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        terminated = False\n",
        "        truncated = done  # Consider ending due to time limit as truncation\n",
        "\n",
        "        # Construct the next observation.\n",
        "        obs = np.array([self.soc, demand, price], dtype=np.float32)\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _compute_reward(self, demand, sold_energy, price, discharge_amount):\n",
        "        \"\"\"\n",
        "        Compute the reward based on the selected reward function.\n",
        "\n",
        "        Three reward functions are available:\n",
        "\n",
        "        1. \"profit\" (default):\n",
        "            Reward = \\( \\text{price} \\times \\text{sold\\_energy} \\)\n",
        "\n",
        "            This function rewards the agent solely based on the revenue generated\n",
        "            from selling surplus energy to the grid after meeting demand.\n",
        "\n",
        "        2. \"penalty_unmet\":\n",
        "            Reward = \\( \\text{price} \\times \\text{discharge\\_amount} - \\lambda \\times \\max(0, \\text{demand} - \\text{discharge\\_amount}) \\)\n",
        "\n",
        "            Here, the agent earns revenue for all discharged energy but is penalized\n",
        "            for any portion of household demand that is not met. This encourages the agent\n",
        "            to ensure that the internal demand is satisfied before selling energy.\n",
        "\n",
        "        3. \"degradation\":\n",
        "            Reward = \\( \\text{price} \\times \\text{sold\\_energy} - c \\times \\text{discharge\\_amount} \\)\n",
        "\n",
        "            This reward takes into account battery degradation. While the agent earns revenue\n",
        "            for selling surplus energy, it incurs a cost proportional to the discharged energy,\n",
        "            which simulates battery wear-and-tear and encourages cautious discharging.\n",
        "        \"\"\"\n",
        "        if self.reward_type == \"profit\":\n",
        "            # --------------------------\n",
        "            # Reward function 1: Profit Reward (default)\n",
        "            # --------------------------\n",
        "            return price * sold_energy\n",
        "\n",
        "        elif self.reward_type == \"penalty_unmet\":\n",
        "            # --------------------------\n",
        "            # Reward function 2: Penalize Unmet Demand.\n",
        "            # --------------------------\n",
        "            penalty = 10.0  # Penalty coefficient (tunable)\n",
        "            unmet_demand = max(0.0, demand - discharge_amount)\n",
        "            return price * discharge_amount - penalty * unmet_demand\n",
        "\n",
        "        elif self.reward_type == \"degradation\":\n",
        "            # --------------------------\n",
        "            # Reward function 3: Battery Degradation Cost Aware.\n",
        "            # --------------------------\n",
        "            degradation_cost = 0.5  # Cost per unit of discharged energy (tunable)\n",
        "            return price * sold_energy - degradation_cost * discharge_amount\n",
        "\n",
        "        else:\n",
        "            # Fallback to default profit reward if an unknown reward type is provided.\n",
        "            return price * sold_energy\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"\n",
        "        Render the current state of the environment.\n",
        "        \"\"\"\n",
        "        print(f\"Step: {self.current_step}, SoC: {self.soc:.2f}\")\n"
      ],
      "metadata": {
        "id": "LabcQU9Mypbo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=6MP1kA74ozKg&format=png&color=000000\" style=\"height:50px;display:inline\"> Agent"
      ],
      "metadata": {
        "id": "AzL3c0inkLXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Replay Buffer\n",
        "# --------------------------\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    A simple Replay Buffer for storing transitions observed from the environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Samples a batch of transitions.\n",
        "        Returns:\n",
        "            state, action, reward, next_state, done: each as a NumPy array.\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Actor Network (Policy)\n",
        "# --------------------------\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"\n",
        "    Actor network for SAC.\n",
        "    Given a state, it outputs the mean and log standard deviation of a Gaussian distribution.\n",
        "    The action is sampled using the reparameterization trick and squashed using tanh.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\n",
        "        super(Actor, self).__init__()\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
        "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "        # Limits for numerical stability\n",
        "        self.LOG_STD_MIN = -20\n",
        "        self.LOG_STD_MAX = 2\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std(x)\n",
        "        # Clamp log_std for numerical stability\n",
        "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state, evaluate=False):\n",
        "        \"\"\"\n",
        "        Sample an action given the state.\n",
        "        If evaluate is True, returns the deterministic action (mean) after squashing.\n",
        "        Otherwise, returns a sampled action along with its log probability.\n",
        "        \"\"\"\n",
        "        mean, log_std = self.forward(state)\n",
        "        std = log_std.exp()\n",
        "\n",
        "        if evaluate:\n",
        "            # For evaluation, use the mean (deterministic policy)\n",
        "            action = torch.tanh(mean) * self.max_action\n",
        "            log_prob = None\n",
        "        else:\n",
        "            # Reparameterization trick\n",
        "            normal = torch.distributions.Normal(mean, std)\n",
        "            x_t = normal.rsample()  # sample and add noise\n",
        "            action = torch.tanh(x_t) * self.max_action\n",
        "\n",
        "            # Compute log probability and adjust for tanh squashing\n",
        "            log_prob = normal.log_prob(x_t)\n",
        "            log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "            # Adjustment term for tanh squashing (to correct probability density)\n",
        "            log_prob -= torch.log(1 - torch.tanh(x_t).pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
        "        return action, log_prob\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Critic Network (Q-function)\n",
        "# --------------------------\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"\n",
        "    Critic network for SAC.\n",
        "    Evaluates the Q-value for a given state and action pair.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.l3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        # Concatenate state and action along the last dimension\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# SAC Agent\n",
        "# --------------------------\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Soft Actor-Critic Agent.\n",
        "    This agent encapsulates the actor, two critics, their target networks, optimizers,\n",
        "    and a replay buffer. It supports automatic entropy tuning for the exploration/exploitation tradeoff.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, args=None):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Hyperparameters (with reasonable default values)\n",
        "\n",
        "        self.gamma = 0.99 if args is None else getattr(args, 'gamma', 0.99)  # Discount factor\n",
        "        self.tau = 0.005 if args is None else getattr(args, 'tau', 0.005)  # Soft update coefficient for target networks\n",
        "        self.batch_size = 256 if args is None else getattr(args, 'batch_size', 256)  # Batch size for updates\n",
        "\n",
        "        # Learning rates\n",
        "        self.actor_lr =3e-14 if args is None else getattr(args, 'actor_lr', 3e-4)\n",
        "        self.critic_lr =3e-14 if args is None else getattr(args, 'critic_lr', 3e-4)\n",
        "\n",
        "        # Replay Buffer capacity\n",
        "        self.buffer_capacity = 1_000_000 if args is None else getattr(args, 'buffer_capacity', 1_000_000)\n",
        "\n",
        "        # Automatic entropy tuning flag and target entropy\n",
        "        self.automatic_entropy_tuning = True if args is None else getattr(args, 'automatic_entropy_tuning', True)\n",
        "        self.target_entropy = -env.action_space.shape[0]\n",
        "\n",
        "        # Environment dimensions\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.max_action = float(env.action_space.high[0])\n",
        "\n",
        "        # Initialize actor and critics (two Qâ€“networks)\n",
        "        self.actor = Actor(self.state_dim, self.action_dim, self.max_action).to(self.device)\n",
        "        self.critic1 = Critic(self.state_dim, self.action_dim).to(self.device)\n",
        "        self.critic2 = Critic(self.state_dim, self.action_dim).to(self.device)\n",
        "\n",
        "        # Create target networks and initialize with critic parameters\n",
        "        self.critic1_target = Critic(self.state_dim, self.action_dim).to(self.device)\n",
        "        self.critic2_target = Critic(self.state_dim, self.action_dim).to(self.device)\n",
        "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
        "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "        # Optimizers\n",
        "        self.actor_optimizer = Adam(self.actor.parameters(), lr=self.actor_lr)\n",
        "        self.critic_optimizer = Adam(list(self.critic1.parameters()) + list(self.critic2.parameters()),\n",
        "                                     lr=self.critic_lr)\n",
        "\n",
        "        # Entropy coefficient (alpha) with optional automatic tuning\n",
        "        if self.automatic_entropy_tuning:\n",
        "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "            self.alpha_optimizer = Adam([self.log_alpha], lr=self.actor_lr)\n",
        "            self.alpha = self.log_alpha.exp().detach()\n",
        "        else:\n",
        "            self.alpha = getattr(args, 'alpha', 0.2)\n",
        "\n",
        "        # Replay Buffer for experience storage\n",
        "        self.replay_buffer = ReplayBuffer(self.buffer_capacity)\n",
        "\n",
        "    def select_action(self, state, evaluate=False):\n",
        "        \"\"\"\n",
        "        Given a state, select an action according to the current policy.\n",
        "        If evaluate is True, return a deterministic action.\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "        action, _ = self.actor.sample(state, evaluate=evaluate)\n",
        "        return action.detach().cpu().numpy()[0]\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Store a transition in the replay buffer.\n",
        "        \"\"\"\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Update the networks (actor and critics) using a mini-batch sampled from the replay buffer.\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None  # Not enough samples to update\n",
        "\n",
        "        # Sample a batch of transitions\n",
        "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        action = torch.FloatTensor(action).to(self.device)\n",
        "        reward = torch.FloatTensor(reward).to(self.device).unsqueeze(1)\n",
        "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "        done = torch.FloatTensor(done).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # --------------------------\n",
        "        # Critic update\n",
        "        # --------------------------\n",
        "        with torch.no_grad():\n",
        "            # Sample next actions and compute their log probabilities\n",
        "            next_action, next_log_prob = self.actor.sample(next_state)\n",
        "            # Compute target Q-values using target networks and take the minimum to mitigate overestimation\n",
        "            target_Q1 = self.critic1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2) - self.alpha * next_log_prob\n",
        "            # Bellman backup for Q functions\n",
        "            target_value = reward + (1 - done) * self.gamma * target_Q\n",
        "\n",
        "        # Compute current Q estimates\n",
        "        current_Q1 = self.critic1(state, action)\n",
        "        current_Q2 = self.critic2(state, action)\n",
        "        # Mean Squared Error loss for both critics\n",
        "        critic_loss = F.mse_loss(current_Q1, target_value) + F.mse_loss(current_Q2, target_value)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # --------------------------\n",
        "        # Actor update\n",
        "        # --------------------------\n",
        "        new_action, log_prob = self.actor.sample(state)\n",
        "        # Evaluate new actions using the current critics\n",
        "        Q1_new = self.critic1(state, new_action)\n",
        "        Q2_new = self.critic2(state, new_action)\n",
        "        Q_new = torch.min(Q1_new, Q2_new)\n",
        "        # The actor loss maximizes the expected return and entropy\n",
        "        actor_loss = (self.alpha * log_prob - Q_new).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # --------------------------\n",
        "        # Entropy coefficient update (if automatic tuning is enabled)\n",
        "        # --------------------------\n",
        "        if self.automatic_entropy_tuning:\n",
        "            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
        "            self.alpha_optimizer.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            self.alpha_optimizer.step()\n",
        "            self.alpha = self.log_alpha.exp().detach()\n",
        "\n",
        "        # --------------------------\n",
        "        # Soft update of target networks\n",
        "        # --------------------------\n",
        "        for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "        for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        # For monitoring purposes, you might return losses\n",
        "        return actor_loss.item(), critic_loss.item()\n"
      ],
      "metadata": {
        "id": "iSqjH7WUyvSP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=122695&format=png&color=000000\" style=\"height:50px;display:inline\"> Trainer"
      ],
      "metadata": {
        "id": "VS73faxqkRUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, env, agent, episodes=100):\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.episodes = episodes\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def train(self):\n",
        "        for episode in range(self.episodes):\n",
        "            state, _ = self.env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0.0\n",
        "\n",
        "            while not done:\n",
        "                action = self.agent.select_action(state)\n",
        "                next_state, reward, done, _, _ = self.env.step(action)\n",
        "                self.agent.store_transition(state, action, reward, next_state, done)\n",
        "                self.agent.update()\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "            self.episode_rewards.append(episode_reward)\n",
        "            writer.add_scalar('Reward/Episode', episode_reward, episode)\n",
        "            print(f\"Episode {episode+1}/{self.episodes} - Reward: {episode_reward:.2f}\")\n",
        "        self.plot_metrics()\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.plot(range(len(self.episode_rewards)), self.episode_rewards)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.title(\"Training Progress\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "UdFl-w3_yyP6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=114903&format=png&color=000000\" style=\"height:50px;display:inline\"> Run"
      ],
      "metadata": {
        "id": "ee2WVwsokU_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "register_env()\n",
        "env = gym.make('ElectricityMarket-v0')\n",
        "agent = Agent(args=None, env=env)\n",
        "trainer = Trainer(env, agent, episodes=50)\n",
        "trainer.train()\n",
        "print(\"Training Complete.\")"
      ],
      "metadata": {
        "id": "Lm6l0Wyoy1H4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=122512&format=png&color=000000\" style=\"height:50px;display:inline\"> Results and Plots"
      ],
      "metadata": {
        "id": "SX4N2lFzkXB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=121704&format=png&color=000000\" style=\"height:50px;display:inline\"> Expension"
      ],
      "metadata": {
        "id": "A2p8WpF5kbUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=122512&format=png&color=000000\" style=\"height:50px;display:inline\"> Results and Plots"
      ],
      "metadata": {
        "id": "kNENCtIVkflQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=4Nd6LXTNhoed&format=png&color=000000\" style=\"height:50px;display:inline\"> Conclusions and Future Work"
      ],
      "metadata": {
        "id": "tn8hiow-koZu"
      }
    }
  ]
}